{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3826aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "# import lazypredict\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import statsmodels.tsa.stattools as st\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771a8a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ger = pd.read_csv('Data/Germany.csv',  index_col='Year')\n",
    "df_ger # the one with categorical variable for Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615eb1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the categorial variables\n",
    "df= df_ger.select_dtypes(include=['float64'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149608c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  plot to see total emission versus year for 'Germany'\n",
    "fig_ger = px.line(df, x=df.index, y=\"Total\")\n",
    "fig_ger.update_xaxes(rangeslider_visible=True)\n",
    "fig_ger.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a651886",
   "metadata": {},
   "source": [
    "Visualize the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749f740",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, dpi=120, figsize=(10,6))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    data = df[df.columns[i]]\n",
    "    ax.plot(data, color='red', linewidth=1)\n",
    "    # Decorations\n",
    "    ax.set_title(df.columns[i])\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8190d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "x = df.index\n",
    "y1 = df['Total']\n",
    "y2 = df['Coal']\n",
    "y3 = df['Oil']\n",
    "y4 = df['Gas']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x,y1)\n",
    "plt.plot(x,y2)\n",
    "plt.plot(x,y3)\n",
    "plt.plot(x,y4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(['Total', 'Coal', 'Oil', 'Gas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be9d06",
   "metadata": {},
   "source": [
    "Grainger's Causality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f380907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "maxlag=12\n",
    "test = 'ssr_chi2test'\n",
    "def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):    \n",
    "    \"\"\"Check Granger Causality of all possible combinations of the Time series.\n",
    "    The rows are the response variable, columns are predictors. The values in the table \n",
    "    are the P-Values. P-Values lesser than the significance level (0.05), implies \n",
    "    the Null Hypothesis that the coefficients of the corresponding past values is \n",
    "    zero, that is, the X does not cause Y can be rejected.\n",
    "\n",
    "    data      : pandas dataframe containing the time series variables\n",
    "    variables : list containing names of the time series variables.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n",
    "            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r, c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "    return df\n",
    "\n",
    "grangers_causation_matrix(df, variables = df.columns)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19284f",
   "metadata": {},
   "source": [
    " Cointegration Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7778a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def cointegration_test(df, alpha=0.05): \n",
    "    \"\"\"Perform Johanson's Cointegration Test and Report Summary\"\"\"\n",
    "    out = coint_johansen(df,-1,5)\n",
    "    d = {'0.90':0, '0.95':1, '0.99':2}\n",
    "    traces = out.lr1\n",
    "    cvts = out.cvt[:, d[str(1-alpha)]]\n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Summary\n",
    "    print('Name   ::  Test Stat > C(95%)    =>   Signif  \\n', '--'*20)\n",
    "    for col, trace, cvt in zip(df.columns, traces, cvts):\n",
    "        print(adjust(col), ':: ', adjust(round(trace,2), 9), \">\", adjust(cvt, 8), ' =>  ' , trace > cvt)\n",
    "\n",
    "cointegration_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192c1ea",
   "metadata": {},
   "source": [
    "Split the Series into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b953b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 10\n",
    "df_train, df_test = df[0:-nobs], df[-nobs:]\n",
    "\n",
    "# Check size\n",
    "print(df_train.shape)  \n",
    "print(df_test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b9eec",
   "metadata": {},
   "source": [
    "Check for Stationarity and Make the Time Series Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb28513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adfuller_test(series, signif=0.05, name='', verbose=False):\n",
    "    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n",
    "    r = adfuller(series, autolag='AIC')\n",
    "    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n",
    "    p_value = output['pvalue'] \n",
    "    def adjust(val, length= 6): return str(val).ljust(length)\n",
    "\n",
    "    # Print Summary\n",
    "    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n",
    "    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n",
    "    print(f' Significance Level    = {signif}')\n",
    "    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n",
    "    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n",
    "\n",
    "    for key,val in r[4].items():\n",
    "        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n",
    "\n",
    "    if p_value <= signif:\n",
    "        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n",
    "        print(f\" => Series is Stationary.\")\n",
    "    else:\n",
    "        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n",
    "        print(f\" => Series is Non-Stationary.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2de168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF Test on each column\n",
    "for name, column in df_train.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ae4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ADF test confirms none of the time series is stationary. Let’s difference all of them once and check again.\n",
    "# 1st difference\n",
    "df_differenced = df_train.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run ADF test on each differenced series.\n",
    "# ADF Test on each column of 1st Differences Dataframe\n",
    "for name, column in df_differenced.iteritems():\n",
    "    adfuller_test(column, name=column.name)\n",
    "    print('\\n')\n",
    "    ### result: all the time series became stationary ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25797e6",
   "metadata": {},
   "source": [
    "How to Select the Order (P) of VAR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a347e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To select the right order of the VAR model, we iteratively fit increasing orders of VAR model and pick the order that gives a model with least AIC.\n",
    "# Though the usual practice is to look at the AIC, you can also check other best fit comparison estimates of BIC, FPE and HQIC.\n",
    "\n",
    "model = VAR(df_differenced)\n",
    "for i in [1,2,3,4,5,6,7,8,9]:\n",
    "    result = model.fit(i)\n",
    "    print('Lag Order =', i)\n",
    "    print('AIC : ', result.aic)\n",
    "    print('BIC : ', result.bic)\n",
    "    print('FPE : ', result.fpe)\n",
    "    print('HQIC: ', result.hqic, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e118c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# An alternate method to choose the order(p) of the VAR models is to use the model.select_order(maxlags) method.\n",
    "#x = model.select_order(maxlags=6)\n",
    "#x.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9d9d2",
   "metadata": {},
   "source": [
    "We choose p = 2 as the number of lags (AIC is locally minimum at p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cce81",
   "metadata": {},
   "source": [
    "Train the VAR Model of Selected Order(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f69ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=2\n",
    "model_fitted = model.fit(p)\n",
    "model_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58acb4",
   "metadata": {},
   "source": [
    "Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic. Results close to 2 is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "def adjust(val, length= 6): return str(val).ljust(length)\n",
    "out = durbin_watson(model_fitted.resid)\n",
    "\n",
    "for col, val in zip(df.columns, out):\n",
    "    print(adjust(col), ':', round(val, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda4d416",
   "metadata": {},
   "source": [
    "How to Forecast VAR model using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d722d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_differenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67dd70d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In order to forecast, the VAR model expects up to the lag order number of observations from the past data.\n",
    "# Get the lag order\n",
    "lag_order = model_fitted.k_ar\n",
    "print(lag_order)  \n",
    "\n",
    "# Input data for forecasting\n",
    "forecast_input = df_differenced.values[-lag_order:]\n",
    "forecast_input # the output is the values of total, coal, oil and gas for 2015 and 2016 (this depends on the train test split and lag_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84380c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forecast\n",
    "fc = model_fitted.forecast(y=forecast_input, steps=nobs) # nobs is from test train split ratio\n",
    "df_forecast = pd.DataFrame(fc, index=df.index[-nobs:], columns=df.columns + '_2d')\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77dfb2",
   "metadata": {},
   "source": [
    "Invert the transformation to get the real forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f36737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_transformation(df_train, df_forecast, second_diff=False):\n",
    "    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n",
    "    df_fc = df_forecast.copy()\n",
    "    columns = df_train.columns\n",
    "    for col in columns:        \n",
    "        # Roll back 2nd Diff\n",
    "        if second_diff:\n",
    "            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n",
    "        # Roll back 1st Diff\n",
    "        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n",
    "    return df_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9e178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_results = invert_transformation(df_train, df_forecast, second_diff=True)        \n",
    "df_results.loc[:, ['Total_forecast', 'Coal_forecast', 'Oil_forecast','Gas_forecast']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4436fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5f38d",
   "metadata": {},
   "source": [
    "Plot of Forecast vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(df.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n",
    "for i, (col,ax) in enumerate(zip(df.columns, axes.flatten())):\n",
    "    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n",
    "    df_test[col][-nobs:].plot(legend=True, ax=ax);\n",
    "    ax.set_title(col + \": Forecast vs Actuals\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines[\"top\"].set_alpha(0)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385d1c19",
   "metadata": {},
   "source": [
    "Evaluate the Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c7a6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To evaluate the forecasts, let’s compute a comprehensive set of metrics, namely, the MAPE, ME, MAE, MPE, RMSE, corr and minmax.\n",
    "from statsmodels.tsa.stattools import acf\n",
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE\n",
    "    me = np.mean(forecast - actual)             # ME\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    mins = np.amin(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    maxs = np.amax(np.hstack([forecast[:,None], \n",
    "                              actual[:,None]]), axis=1)\n",
    "    minmax = 1 - np.mean(mins/maxs)             # minmax\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'corr':corr, 'minmax':minmax})\n",
    "\n",
    "print('Forecast Accuracy of: Total')\n",
    "accuracy_prod = forecast_accuracy(df_results['Total_forecast'].values, df_test['Total'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,4))\n",
    "\n",
    "print('\\nForecast Accuracy of: Coal')\n",
    "accuracy_prod = forecast_accuracy(df_results['Coal_forecast'].values, df_test['Coal'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,4))\n",
    "\n",
    "print('\\nForecast Accuracy of: Oil')\n",
    "accuracy_prod = forecast_accuracy(df_results['Oil_forecast'].values, df_test['Oil'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,4))\n",
    "\n",
    "print('\\nForecast Accuracy of: Gas')\n",
    "accuracy_prod = forecast_accuracy(df_results['Gas_forecast'].values, df_test['Gas'])\n",
    "for k, v in accuracy_prod.items():\n",
    "    print(adjust(k), ': ', round(v,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70eb5b6",
   "metadata": {},
   "source": [
    "Forecasting the future: for 2 years (since p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_differenced_all = df.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5beec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_differenced_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a867253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_input_all = df_differenced_all.values[-lag_order:]\n",
    "forecast_input_all # the output is the values of total, coal, oil and gas for 2015 and 2016 (this depends on the train test split and lag_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25140568",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_all = VAR(df_differenced_all)\n",
    "model_fitted_all = model_all.fit(p)\n",
    "model_fitted_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "\n",
    "fc_all = model_fitted_all.forecast(y=forecast_input_all, steps=nobs) # nobs is from test train split ratio\n",
    "df_forecast_all = pd.DataFrame(fc_all, index=[2022,2023,2024,2025,2026], columns=df.columns + '_2d')\n",
    "df_forecast_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41b6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
